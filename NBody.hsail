module &_tmp_cloc2112_NBody_opt_bc:1:0:$full:$large:$default;
extension "amd:gcn";
extension "IMAGE";

decl prog function &__printf_id(arg_u32 %ret)();

decl prog function &abort()();

prog kernel &__OpenCL_nbody_kern_kernel(
	kernarg_u64 %__global_offset_0,
	kernarg_u64 %__global_offset_1,
	kernarg_u64 %__global_offset_2,
	kernarg_u64 %__printf_buffer,
	kernarg_u64 %__vqueue_pointer,
	kernarg_u64 %__aqlwrap_pointer,
	kernarg_f32 %dt1,
	kernarg_f32 %eps,
	kernarg_u64 %pos_old,
	kernarg_u64 %pos_new,
	kernarg_u64 %vel,
	kernarg_u64 %pblock)
{
	pragma "AMD RTI", "ARGSTART:__OpenCL_nbody_kern_kernel";
	pragma "AMD RTI", "version:3:1:104";
	pragma "AMD RTI", "device:generic";
	pragma "AMD RTI", "uniqueid:1024";
	pragma "AMD RTI", "memory:private:16";
	pragma "AMD RTI", "memory:region:0";
	pragma "AMD RTI", "memory:local:0";
	pragma "AMD RTI", "printf_fmt:1:1:24:%s\n";
	pragma "AMD RTI", "value:__global_offset_0:u64:1:1:0";
	pragma "AMD RTI", "value:__global_offset_1:u64:1:1:16";
	pragma "AMD RTI", "value:__global_offset_2:u64:1:1:32";
	pragma "AMD RTI", "pointer:__printf_buffer:u8:1:1:48:uav:7:1:RW:0:0:0";
	pragma "AMD RTI", "value:__vqueue_pointer:u64:1:1:64";
	pragma "AMD RTI", "value:__aqlwrap_pointer:u64:1:1:80";
	pragma "AMD RTI", "value:dt1:float:1:1:96";
	pragma "AMD RTI", "value:eps:float:1:1:112";
	pragma "AMD RTI", "pointer:pos_old:float:1:1:128:uav:7:16:RW:0:0:0";
	pragma "AMD RTI", "pointer:pos_new:float:1:1:144:uav:7:16:RW:0:0:0";
	pragma "AMD RTI", "pointer:vel:float:1:1:160:uav:7:16:RW:0:0:0";
	pragma "AMD RTI", "pointer:pblock:float:1:1:176:l:7:16:RW:0:0:0";
	pragma "AMD RTI", "function:1:0";
	pragma "AMD RTI", "memory:64bitABI";
	pragma "AMD RTI", "privateid:8";
	pragma "AMD RTI", "enqueue_kernel:0";
	pragma "AMD RTI", "kernel_index:0";
	pragma "AMD RTI", "reflection:0:size_t";
	pragma "AMD RTI", "reflection:1:size_t";
	pragma "AMD RTI", "reflection:2:size_t";
	pragma "AMD RTI", "reflection:3:size_t";
	pragma "AMD RTI", "reflection:4:size_t";
	pragma "AMD RTI", "reflection:5:size_t";
	pragma "AMD RTI", "reflection:6:float";
	pragma "AMD RTI", "reflection:7:float";
	pragma "AMD RTI", "reflection:8:float4*";
	pragma "AMD RTI", "reflection:9:float4*";
	pragma "AMD RTI", "reflection:10:float4*";
	pragma "AMD RTI", "reflection:11:float4*";
	pragma "AMD RTI", "ARGEND:__OpenCL_nbody_kern_kernel";
	align(4) private_u8 %__privateStack[4];

@__OpenCL_nbody_kern_kernel_entry:
	// BB#0:                                // %.split
	ld_kernarg_align(8)_width(all)_u64	$d4, [%__printf_buffer];
	ld_global_align(4)_const_width(all)_u32	$s3, [$d4+4];
	atomic_ld_global_scacq_agent_b32	$s5, [$d4];
	ld_kernarg_align(8)_width(all)_u64	$d1, [%pblock];
	ld_kernarg_align(8)_width(all)_u64	$d3, [%vel];
	ld_kernarg_align(8)_width(all)_u64	$d0, [%pos_new];
	ld_kernarg_align(8)_width(all)_u64	$d2, [%pos_old];
	st_private_align(4)_u32	$s5, [%__privateStack];
	ld_kernarg_align(4)_width(all)_f32	$s1, [%eps];
	ld_kernarg_align(4)_width(all)_f32	$s0, [%dt1];
	mov_b32	$s2, 0;
	mov_b32	$s6, 1;

@BB0_1:
	add_u32	$s4, $s5, 36;
	cmp_gt_b1_u32	$c0, $s4, $s3;
	cbr_b1	$c0, @BB0_4;
	// BB#2:
	add_u32	$s5, $s5, 28;
	ld_private_align(4)_u32	$s4, [%__privateStack];
	atomic_cas_global_scar_agent_b32	$s5, [$d4], $s4, $s5;
	st_private_align(4)_u32	$s5, [%__privateStack];
	cmp_ne_b1_s32	$c0, $s5, $s4;
	cbr_b1	$c0, @BB0_1;
	// BB#3:
	mov_b32	$s2, $s6;

@BB0_4:
	// %unified_loop_exit
	cmp_eq_b1_s32	$c0, $s2, 0;
	cbr_b1	$c0, @BB0_8;
	// BB#5:                                // %unified_loop_exit
	cmp_ne_b1_s32	$c0, $s2, 1;
	cbr_b1	$c0, @BB0_8;
	// BB#6:                                // %__printf_alloc.exit
	cvt_u64_u32	$d5, $s4;
	add_u64	$d4, $d5, $d4;
	add_u64	$d5, $d4, 8;
	nullptr_u64	$d6;
	cmp_eq_b1_s64	$c0, $d5, $d6;
	cbr_b1	$c0, @BB0_8;
	// BB#7:
	st_v4_global_align(4)_u32	(1, 1936287860, 544434464, 1702109281), [$d5];
	st_v3_global_align(4)_u32	(1931506803, 1852404340, 2663), [$d4+24];

@BB0_8:
	// %__printf_alloc.exit.thread
	workitemabsid_u32	$s2, 0;
	cvt_u64_u32	$d4, $s2;
	ld_kernarg_align(8)_width(all)_u64	$d5, [%__global_offset_0];
	add_u64	$d4, $d4, $d5;
	shl_u64	$d4, $d4, 32;
	shr_s64	$d4, $d4, 32;
	shl_u64	$d4, $d4, 4;
	add_u64	$d5, $d2, $d4;
	add_u64	$d3, $d3, $d4;
	mov_b32	$s2, 0;
	currentworkgroupsize_u32	$s15, 0;
	gridsize_u32	$s3, 0;
	div_s32	$s16, $s3, $s15;
	ld_v4_global_align(16)_f32	($s6, $s5, $s4, $s3), [$d3];
	ld_v4_global_align(16)_f32	($s10, $s9, $s8, $s7), [$d5];
	cmp_lt_b1_s32	$c0, $s16, 1;
	cbr_b1	$c0, @BB0_9;
	// BB#10:                                // %.lr.ph10
	workitemid_u32	$s17, 0;
	cvt_s64_s32	$d5, $s17;
	mov_b32	$s11, 0;
	mov_b32	$s18, 0;
	shl_u64	$d5, $d5, 4;
	add_u64	$d5, $d1, $d5;
	mov_b32	$s14, $s11;
	mov_b32	$s12, $s11;
	mov_b32	$s13, $s11;

@BB0_11:
	mad_u32	$s19, $s18, $s15, $s17;
	cvt_s64_s32	$d6, $s19;
	shl_u64	$d6, $d6, 4;
	add_u64	$d6, $d2, $d6;
	ld_v4_global_align(16)_f32	($s19, $s20, $s21, $s22), [$d6];
	cvt_u32_u64	$s23, $d5;
	st_v4_group_align(16)_f32	($s19, $s20, $s21, $s22), [$s23];
	barrier;
	cmp_lt_b1_s32	$c0, $s15, 1;
	mov_b64	$d6, $d1;
	currentworkgroupsize_u32	$s19, 0;
	cbr_b1	$c0, @BB0_13;

@BB0_12:
	// %.lr.ph
	cvt_u32_u64	$s24, $d6;
	ld_v4_group_align(16)_width(WAVESIZE)_f32	($s21, $s20, $s23, $s22), [$s24];
	sub_ftz_f32	$s20, $s20, $s9;
	mul_ftz_f32	$s24, $s20, $s20;
	sub_ftz_f32	$s21, $s21, $s10;
	mul_ftz_f32	$s25, $s21, $s21;
	add_ftz_f32	$s24, $s25, $s24;
	sub_ftz_f32	$s23, $s23, $s8;
	mul_ftz_f32	$s25, $s23, $s23;
	add_ftz_f32	$s24, $s25, $s24;
	add_ftz_f32	$s24, $s24, $s1;
	add_u64	$d6, $d6, 16;
	add_u32	$s19, $s19, 4294967295;
	nrsqrt_f32	$s24, $s24;
	mul_ftz_f32	$s26, $s22, $s24;
	sub_ftz_f32	$s25, $s22, $s7;
	mul_ftz_f32	$s22, $s24, $s26;
	mul_ftz_f32	$s22, $s24, $s22;
	mul_ftz_f32	$s24, $s25, $s22;
	add_ftz_f32	$s13, $s13, $s24;
	mul_ftz_f32	$s23, $s23, $s22;
	add_ftz_f32	$s12, $s12, $s23;
	mul_ftz_f32	$s20, $s20, $s22;
	add_ftz_f32	$s14, $s14, $s20;
	mul_ftz_f32	$s20, $s21, $s22;
	add_ftz_f32	$s11, $s11, $s20;
	cmp_ne_b1_s32	$c0, $s19, 0;
	cbr_b1	$c0, @BB0_12;

@BB0_13:
	// %._crit_edge
	barrier;
	add_u32	$s18, $s18, 1;
	cmp_ne_b1_s32	$c0, $s18, $s16;
	cbr_b1	$c0, @BB0_11;
	br	@BB0_14;

@BB0_9:
	mov_b32	$s11, $s2;
	mov_b32	$s14, $s2;
	mov_b32	$s12, $s2;
	mov_b32	$s13, $s2;

@BB0_14:
	// %._crit_edge11
	mul_ftz_f32	$s1, $s0, 0F3f000000;
	mul_ftz_f32	$s15, $s0, 0F3f000000;
	mul_ftz_f32	$s16, $s0, $s15;
	mul_ftz_f32	$s15, $s0, $s1;
	mul_ftz_f32	$s1, $s0, $s6;
	mul_ftz_f32	$s15, $s15, $s11;
	mul_ftz_f32	$s17, $s0, $s5;
	mul_ftz_f32	$s18, $s16, $s14;
	mul_ftz_f32	$s16, $s0, $s4;
	mul_ftz_f32	$s19, $s0, 0F3f000000;
	mul_ftz_f32	$s19, $s0, $s19;
	mul_ftz_f32	$s19, $s19, $s12;
	add_ftz_f32	$s16, $s16, $s19;
	add_ftz_f32	$s17, $s17, $s18;
	add_ftz_f32	$s1, $s1, $s15;
	mul_ftz_f32	$s18, $s2, 0F3f000000;
	mul_ftz_f32	$s15, $s2, $s3;
	mul_ftz_f32	$s18, $s2, $s18;
	add_ftz_f32	$s1, $s10, $s1;
	add_ftz_f32	$s9, $s9, $s17;
	add_ftz_f32	$s8, $s8, $s16;
	mul_ftz_f32	$s10, $s0, $s11;
	mul_ftz_f32	$s11, $s18, $s13;
	add_ftz_f32	$s11, $s15, $s11;
	add_ftz_f32	$s7, $s7, $s11;
	add_ftz_f32	$s6, $s6, $s10;
	mul_ftz_f32	$s10, $s0, $s14;
	add_u64	$d0, $d0, $d4;
	st_v4_global_align(16)_f32	($s1, $s9, $s8, $s7), [$d0];
	add_ftz_f32	$s1, $s5, $s10;
	mul_ftz_f32	$s0, $s0, $s12;
	add_ftz_f32	$s0, $s4, $s0;
	mul_ftz_f32	$s2, $s2, $s13;
	add_ftz_f32	$s2, $s3, $s2;
	st_v4_global_align(16)_f32	($s6, $s1, $s0, $s2), [$d3];
	ret;
};
